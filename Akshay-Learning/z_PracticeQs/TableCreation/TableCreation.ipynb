{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdc9ebc-a3ea-4806-bd9b-8189b5d30fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"File_name\",\"BLANK\",\"Enter File name\")\n",
    "File_name = dbutils.widgets.get(\"File_name\")\n",
    "print(f\"File_name: {File_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77139c2e-4c1c-4116-b547-6d8b3f6b8902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"Database_name\",\"BLANK\",\"Enter Database name\")\n",
    "Database_name = dbutils.widgets.get(\"Database_name\")\n",
    "print(f\"Database_name: {Database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "274dafe5-b82d-4e8d-9914-1bf3c93b1281",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3992a5dc-c661-49d6-b982-2c9df9ecd8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c8cf33-e1df-49bb-8c5a-8fa40ae36e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Table_Detail = pd.read_csv(File_name)\n",
    "display(Table_Detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5dbb024-cf0a-4f7c-b6ea-ebb7afbdfc49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pandas approach"
    }
   },
   "outputs": [],
   "source": [
    "Table_Detail['Column_List'] = Table_Detail['Column_Name'] + ' ' + Table_Detail['Data_Type']\n",
    "Table_Detail_new = Table_Detail[['Table_Name','Column_List']]\n",
    "display(Table_Detail_new)\n",
    "\n",
    "Table_Detail_new2 = Table_Detail_new\\\n",
    ".groupby('Table_Name')['Column_List']\\\n",
    ".apply(lambda x: '(' + ','.join(x.astype(str)) + ')')\\\n",
    ".reset_index()\n",
    "display(Table_Detail_new2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73be943-5a0b-4098-b14b-b3b467bb4aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def createTable(Table_Name, Column_List):\n",
    "    spark.sql(f\"create database if not exists {Database_name}\")\n",
    "    spark.sql(f\"drop table if exists {Database_name}.{Table_Name}\")\n",
    "    spark.sql(f\"create table {Database_name}.{Table_Name} {Column_List}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f385acd7-b926-41da-a183-b4049f485dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for index, row in Table_Detail_new2.iterrows():\n",
    "    createTable(row['Table_Name'], row['Column_List'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca122e9-6936-4c53-9e93-e8d2c3195dda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark approach"
    }
   },
   "outputs": [],
   "source": [
    "#PySpark implements distributed data processing (consumes more resources). Thus, use PySpark only for huge data. \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "Table_Detail['Column_List'] = Table_Detail['Column_Name'] + ' ' + Table_Detail['Data_Type']\n",
    "Table_Detail_new = spark.createDataFrame(Table_Detail[['Table_Name','Column_List']]) \n",
    "display(Table_Detail_new)\n",
    "\n",
    "Table_Detail_new2 = Table_Detail_new.groupby('Table_Name')\\\n",
    ".agg(F.concat_ws(',',F.collect_list('Column_List'))\\\n",
    "     .alias('Column_List'))\\\n",
    "     .withColumn('Column_List', F.concat(F.lit(\"(\"), F.col(\"Column_List\"), F.lit(\")\")))\n",
    "display(Table_Detail_new2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Database_name",
      "width": 171
     },
     {
      "breakBefore": false,
      "name": "File_name",
      "width": 134
     }
    ]
   },
   "notebookName": "TableCreation",
   "widgets": {
    "Database_name": {
     "currentValue": "TableCreation",
     "nuid": "3952e933-4ef5-4ae4-a45b-4ce68596e47e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "BLANK",
      "label": "Enter Database name",
      "name": "Database_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "BLANK",
      "label": "Enter Database name",
      "name": "Database_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "File_name": {
     "currentValue": "/Workspace/Users/akshay.chidrawar@ltimindtree.com/DatabricksLearning/TableCreation/Table_Detail.csv",
     "nuid": "174a299d-5e60-4a10-b8d4-5f9aa43ad18e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "BLANK",
      "label": "Enter File name",
      "name": "File_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "BLANK",
      "label": "Enter File name",
      "name": "File_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
