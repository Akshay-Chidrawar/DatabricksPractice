{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c2344c90-9a3a-47e8-9152-deeaac7e5f3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read JSON data"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "directory = '03/demos/coffee_sales/'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "             data = json.load(f)\n",
    "             print('File:', filename)\n",
    "             print('Type:', type(data))\n",
    "\n",
    "             if isinstance(data, list):\n",
    "                 print('List Items:')\n",
    "                 for item in data:\n",
    "                     print(item)\n",
    "             elif isinstance(data, dict):\n",
    "                 print('Dictionary Items:')\n",
    "                 for key, value in data.items():\n",
    "                     print(key, ':', value)\n",
    "             else:\n",
    "                 print('Data:')\n",
    "                 print(data)\n",
    "\n",
    "             print('-------------------------')\n",
    "\n",
    "with open('03/demos/coffee_sales/coffee_sales.json', 'r') as f:\n",
    "    file_data = json.load(f)\n",
    "\n",
    "json_string = json.dumps(file_data)\n",
    "json_data = json.loads(json_string)\n",
    "\n",
    "locations_data = json_data['locations']\n",
    "sales_data = json_data['sales']\n",
    "\n",
    "locations_names = [location['name'] for location in locations_data]\n",
    "sales_dates = [sale['date'] for sale in sales_data]\n",
    "sales_sales = [sale['sales'] for sale in sales_data]\n",
    "\n",
    "coffee_data = {'locations_names': locations_names, 'sales_dates': sales_dates, 'sales_sales': sales_sales}\n",
    "\n",
    "print('\\nfile_data:',type(file_data),'\\n')\n",
    "display(file_data)\n",
    "print('\\njson_string:',type(json_string),'\\n')\n",
    "display(json_string)\n",
    "print('\\njson_data:',type(json_data),'\\n')\n",
    "display(json_data)\n",
    "print('\\nfile_data same as json_data? ',file_data==json_data,'\\n')\n",
    "print('\\ncoffee_data:',type(coffee_data),'\\n')\n",
    "display(coffee_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d783396-f9ff-4764-ae40-05783811d061",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Python + Spark SQL (not entirely correct)"
    }
   },
   "outputs": [],
   "source": [
    "directory = '/Volumes/workspace/default/managed_volume/ManishKumar/'\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#Custom way to convert multiline json to singleline json. Below code snippet will:\n",
    "\n",
    "# 1. read entire json file (multiline) as a single column 'value', with each line on a new row.\n",
    "# 2. remove characters for new line, tabs and white spaces (indentation)\n",
    "# 3a. collect all rows (RowObjects) into a list using collect_list().\n",
    "# 3b. concat all rows into a single string using concat_ws().\n",
    "# 4. capture string contents of 3b in a Python string variable.\n",
    "\n",
    "payload = spark.read.format('text')\\\n",
    "        .options(multiline=True,mode='permissive')\\\n",
    "        .load(directory+'json_data.json')\\\n",
    "        .withColumn('value',regexp_replace(regexp_replace(regexp_replace('value', '\\n', ''), '\\t', ''),'  ',''))\\\n",
    "        .select(concat_ws('',collect_list('value')).alias('json_contents'))\n",
    "display(payload)\n",
    "\n",
    "# 4a. Using collect(), create a a python list of Row objects \"list_of_RowObjects\" from a spark dataframe \"payload\". Both will contain same data, but representation is different.\n",
    "# 4b. Using row iterator [n], extract the first row object from the list and assign it to a row object variable \"row\".\n",
    "# 4c. Using row['column'] to extract the actual string contents of the \"column\".\n",
    "list_of_RowObjects = payload.collect()\n",
    "row = list_of_RowObjects[0]\n",
    "json_contents_string = row['json_contents']\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "        create or replace temporary view vw_json as\n",
    "        select  json_data.*\n",
    "        from    (\n",
    "                select  from_json(\n",
    "                        '{json_contents_string}'\n",
    "                        ,schema_of_json('{json_contents_string}')\n",
    "                        ) as json_data\n",
    "                )\n",
    "        \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"select * from vw_json\"\"\").display()\n",
    "# #this gives json string. Copy only 1st record from results and paste in next snippet.\n",
    "\n",
    "#sample json string for schema inference\n",
    "json_string = '{\"user_id\":\"0001\",\"first_name\":\"Akshay\",\"listings\": [{\"listing_id\":\"847254\",\"place\": {\"Area\":\"Naupada\",\"City\":\"Thane\"},\"description\":\"apartment\",\"services\":[{\"service_id\":\"BG111\",\"service_type\":\"CookingGas\",\"service_provider\":\"BharatGas\"},{\"service_id\":\"MV111\",\"service_type\":\"Electricity\",\"service_provider\":\"Mahavitaran\"}]},{\"listing_id\":\"435543\",\"place\": {\"Area\":\"ShivajiNagar\",\"City\":\"Pune\"},\"description\":\"vila\",\"services\":[{\"service_id\":\"HG111\",\"service_type\":\"CookingGas\",\"service_provider\":\"HidustanGas\"},{\"service_id\":\"RL111\",\"service_type\":\"Electricity\",\"service_provider\":\"Reliance\"}]}]}'\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "        create or replace temporary view vw_json as\n",
    "        select  from_json('{json_string}'\n",
    "                ,schema_of_json('{json_string}')) as json_data\n",
    "        \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"select * from vw_json\"\"\").display()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select  * except (services)\n",
    "            ,services.service_id as service_id\n",
    "            ,services.service_provider as service_provider\n",
    "            ,services.service_type as service_type\n",
    "    from    (\n",
    "            select  * except (listings)\n",
    "                    ,listings.listing_id as listing_id\n",
    "                    ,listings.description as description\n",
    "                    ,listings.place.Area as Area\n",
    "                    ,listings.place.City as City\n",
    "                    ,explode(listings.services) as services\n",
    "            from    (\n",
    "                        select  * except (json_data) \n",
    "                                ,json_data.user_id as user_id\n",
    "                                ,json_data.first_name as first_name\n",
    "                                ,explode(json_data.listings) as listings\n",
    "                        from  vw_json\n",
    "                    )t1\n",
    "            )t2\n",
    "    order by user_id, listing_id, service_id\n",
    "    \"\"\").display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Archive",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
